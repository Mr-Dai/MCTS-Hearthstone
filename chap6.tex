\chapter{实验结果}
\label{section:experimentalResults}

本章为论文的实验结果部分。本论文的实验将测试~\ref{section:RuleBasedAIPlayers}~节和第~\ref{section:MonteCarloPlayers}~章提到的四种智能体，分别为随机智能体、专家规则智能体和两种蒙特卡洛搜索智能体。
智能体将使用同一副卡组进行对弈，以免不同卡组之间的强度差异导致实验结果发生偏移。测试卡组的组成已在表~\ref{table:TestDeck}~中给出。
在~\ref{section:Test1}~节，四种智能体首先将进行两两对弈并给出各自的胜场数，以让读者对不同智能体的平均决策强度差异有大概的认知；
在~\ref{section:Test2}~节中，\ref{section:RandomRuleBased}~节提及的随机规则智能体将用作蒙特卡洛智能体的模拟策略，以测试启发式的先验知识对蒙特卡洛智能体决策强度的影响。

\begin{table}[!ht]
\small
\center
\caption{测试智能体}
\label{table:TestAi}
\begin{tabular}{|l|l|}
\hline
智能体名称    & 智能体描述                                       \\
\hline
Random        & 完全随机的智能体                                 \\
\hline
Rule-based    & 基于预定义规则的智能体                           \\
\hline
MC Random     & 使用随机智能体进行模拟的基于蒙特卡洛搜索的智能体 \\
\hline
MC Rule-based & 使用规则智能体进行模拟的基于蒙特卡洛搜索的智能体 \\
\hline
\end{tabular}
\end{table}

\section{智能体之间的决策强度差异}
\label{section:Test1}

本实验将用于测试不同智能体之间的决策强度的差异。智能体们将会进行相互对战。每组智能体各进行~200~次对战，并最终给出各个智能体在各种不同的组合中的胜场数。
其中，蒙特卡洛智能体默认在对所有操作进行一次尝试后会额外进行~500~次迭代。
测试时使用的计算机\footnote{Intel Core i5-3210M @ 2.50GHz 2.50GHz}可以在~1~秒内完成~500~次的模拟游戏。
后面的实验结果也表明~500~次的迭代已足以让蒙特卡洛智能体做出足够强度的决策。
本实验的结果也将用作后续实验的基准数据。

\begin{table}[!ht]
\small
\center
\caption{智能体对弈胜场数}
\label{table:Test1Results}
\begin{tabular}{|l|c|c|c|c|}
\hline
VS.           & Random & Rule-based & MC Random & MC Rule-based \\
\hline
Random        & 100    & 2          & 0         & 0             \\
\hline
Rule-based    & 198    & 100        & 88        & 60            \\
\hline
MC Random     & 200    & 112        & 100       & 140           \\
\hline
MC Rule-based & 200    & 140        & 60        & 100           \\
\hline
\end{tabular}
\end{table}

表~\ref{table:Test1Results}~给出了此次实验的结果。不难看出，随机智能体比起规则智能体和蒙特卡洛智能体都弱得多。
基于随机模拟的蒙特卡洛智能体在面对规则智能体时可以有~56\%~的胜率，可见即使是未添加任何先验知识的蒙特卡洛智能体已能与拥有大量先验知识的规则智能体并驾齐驱。
值得注意的是，当采用规则智能体进行模拟时，蒙特卡洛智能体拥有了来自规则智能体的先验知识，同时也拥有了蒙特卡洛搜索返回最优解的保证，
因此可以看到，基于规则模拟的蒙特卡洛智能体在面对规则智能体时的胜率比起基于随机模拟的蒙特卡洛智能体有了显著的提升（提升至~70\%）。
更为有趣的是，基于规则模拟的蒙特卡洛智能体与基于随机模拟的蒙特卡洛智能体直接对战时，基于规则的智能体反而处于劣势。

\section{迭代次数与启发知识对蒙特卡洛智能体决策强度的影响}
\label{section:Test2}

由~\ref{section:MonteCarloSearch}~节可知，蒙特卡洛搜索通过不断地重复采样以近似每个操作的期望收益，采样次数越多意味着得到的期望收益越准确。
该过程类似于近似值逐渐逼近正确值的过程，理论上可知当采样次数达到一定程度后，采样次数的提高所能带来的可靠性的提高会变得微乎其微。
本次实验将调整上述实验中使用的两个蒙特卡洛智能体的迭代次数，以测试蒙特卡洛智能体在模拟次数逐渐增加时会有怎样的表现。

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{img/Fig4-1.jpg}
\caption{基于随机规则模拟的蒙特卡洛智能体~vs.~规则智能体}
\label{fig:41}
\end{figure}

图~\ref{fig:41}~为随机规则模拟的蒙特卡洛智能体与规则智能体进行对战时，蒙特卡洛智能体的胜率的变化。
其中每个结点代表的数据为蒙特卡洛智能体在给定的迭代次数和~$p$~值下，与专家规则智能体进行~500~场随机对弈后蒙特卡洛智能体的胜率。
当~$p = 1$~时，随机规则智能体的行为与规则智能体的行为是完全一致的；当~$p = 0$~时，随机规则智能体的行为与随机智能体的行为是完全一致的。
尽管变化不是很大，但仍能清楚地看到，在~$p = 1$~时，随着迭代次数的增多，蒙特卡洛智能体的胜场数有上升的趋势。
蒙特卡洛智能体的胜率在迭代数超过~100~后趋于稳定，稳定在了~67\%~胜率附近。这个数字与~\ref{section:Test1}~节得出的胜率颇为接近。
与之形成对比的是，当~$p = 0$~时，蒙特卡洛智能体的表现在迭代数较低的情况下较差，当迭代次数达到~100~以上时蒙特卡洛智能体的胜率才达到了~50\%~。
随着迭代数的增加，蒙特卡洛智能体的决策强度仍在不断增加，当迭代数达到~2000~时，蒙特卡洛智能体的胜率已达到~60\%~，与~$p = 1$~时的表现比较接近。
除此之外，图中还给出了当~$p = 0.5$~时蒙特卡洛智能体的胜率的走势。有趣的是，在迭代数达到~100~时，蒙特卡洛智能体的胜率已达到了与~$p=1$~时相同的水平，
而且仍在不断上升，最终超过了~$p=1$~时的表现，并在迭代数达到~2000~时达到了~75\%~以上的胜率。

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{img/Fig4-2.jpg}
\caption{基于规则模拟的蒙特卡洛智能体~vs.~基于随机模拟的蒙特卡洛智能体}
\label{fig:42}
\end{figure}

图~\ref{fig:42}~为~$p=1$~的蒙特卡洛智能体与~$p=0$~的蒙特卡洛智能体进行对战时，~$p=1$~的蒙特卡洛智能体的胜场数的变化。
与猜想的结果截然不同的是，随着两个智能体的迭代次数的增加，~$p=1$~的蒙特卡洛智能体的胜场数在逐渐下降。
当迭代次数达到~500~以上时，其胜率已经下降到了~35\%~附近。

以上几次对弈测试的结果显示出了十分有趣的规律。首先比起随机蒙特卡洛智能体，规则蒙特卡洛智能体在与规则智能体对弈时，
其胜率可以以更少的迭代次数（100）稳定在较高的胜率（65\%）。尽管随机蒙特卡洛智能体表现不如规则蒙特卡洛智能体，
但随机蒙特卡洛智能体也在一定的迭代次数后（500），胜场数也趋于稳定（63\%）。
当~$p=0.5$~时，蒙特卡洛智能体的胜率并未止步于~65\%~，而是一路攀升，最终全面超越了其在~$p=1$~时的表现。
除此之外，当两个蒙特卡洛智能体进行相互对弈时，~$p=1$~的蒙特卡洛智能体由于启发知识的加入反而输给了~$p=0$~的蒙特卡洛智能体。

蒙特卡洛搜索算法本身无法对对手的行为作出任何预测。完全随机的蒙特卡洛智能体由于不包含任何启发知识，在无法对对手的行为作出精准预测的情况下，
它得出的操作序列的期望收益与现实情况可能是不匹配的。但随着迭代次数的增加，大量的随机采样往往能保证期望收益最终趋近于真实值。
大概也正因如此，随机模拟的蒙特卡洛智能体需要更多的迭代来让胜场数趋于稳定，稳定后的胜场数也与~$p=1$~的蒙特卡洛智能体较为接近。
当~$p=1$~时，蒙特卡洛智能体由于加入了启发知识，它能更好地预测规则智能体的行为。但正是由于启发知识的加入，蒙特卡洛智能体的迭代采样不是随机的，
这就导致了它得出的期望收益是有偏差的，不适用于所有类型的对手，而完全随机的蒙特卡洛智能体没有这样的局限。因此随着迭代次数的增加，
规则蒙特卡洛智能体渐渐被随机蒙特卡洛智能体超越，且差距越来越大。
