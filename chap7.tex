\chapter{结语}

\section{结论}

本文在第~\ref{section:Preliminaries}~章详细介绍了蒙特卡洛搜索和蒙特卡洛树搜索的基本定义，并在第~\ref{section:HearthstoneRule}~章介绍了炉石传说的基本规则。
第~\ref{section:RuleBasedAIPlayers}~章给出的本文实验的基本测试环境，并给出了测试中使用的规则智能体的详细定义。
第~\ref{section:MonteCarloPlayers}~章给出了蒙特卡洛搜索与蒙特卡洛树搜索应用于炉石传说的基本方法，
在~\ref{section:MonteCarloTreeSearchPlayer}~节末尾讨论了将蒙特卡洛树搜索直接应用于炉石传说所遇到的瓶颈，并在~\ref{section:Optimization}~节给出了可进行的优化。

第~\ref{section:experimentalResults}~章构建了四个不同的炉石传说智能体并使其进行两两对弈。最终得知，本文使用的规则智能体比普通的随机智能体有强得多的决策强度。
完全不包含先验知识的随机模拟的蒙特卡洛智能体能够轻易击败最弱的随机智能体，与拥有大量先验知识的规则智能体对弈时亦能拥有不错的表现。
基于规则模拟的蒙特卡洛智能体加入了规则智能体所拥有的先验知识，在决策强度上有了一定的提升，能比随机蒙特卡洛智能体更快趋于稳定，稳定后的胜率也比随机蒙特卡洛智能体稍高。
这意味着，蒙特卡洛搜索的加入能够提高智能体的决策强度，规则智能体的先验知识的加入同样能够提高智能体的决策强度，而先验知识的加入与蒙特卡洛搜索算法之间并不冲突。
蒙特卡洛搜索意味着无启发知识的搜索型智能体，规则智能体则意味着依赖启发知识的启发型智能体。
它们代表着截然不同的人工智能研究方向，而两者相结合所带来的成功意味深远。
不幸的是，先验知识的加入导致规则蒙特卡洛智能体的表现发生了偏差：它能比随机蒙特卡洛智能体更好地应对规则智能体，但在面对其他类型的对手时的表现却有所下降。

除此之外，实验部分还验证了迭代次数对蒙特卡洛智能体决策强度的影响。可见，对于炉石传说而言，500~次的蒙特卡洛迭代已足以让蒙特卡洛搜索算法近似求得各个操作的期望收益，
更多的迭代已不能继续得到更好的结果。考虑到~500~次的迭代对现代计算机而言只需要~1~秒的处理时间，蒙特卡洛人工智能在性能上也是可以接受的。

\section{未来的工作方向}

蒙特卡洛搜索固然能带来智能体决策强度的提高，但先验知识的加入对蒙特卡洛智能体的提升也是显而易见的。
炉石传说人工智能仍然可以通过各种方式加入更多的先验知识：无论是基于规则的智能体，还是利用学习算法使得智能体拥有学习能力，
本文的实验结果都确保了它们的加入能够更好地模拟对手的行为，为智能体的决策强度带来提高。
在炉石传说这样的不完全信息游戏中，对手行为预测是十分重要的。而要对对手进行预测，加入启发知识并不是唯一的方法。
实验中，规则模拟与蒙特卡洛搜索算法的结合所带来的决策强度的提升，也预示了基于规则与基于蒙特卡洛方法这两个方向的人工智能研究并不是相互矛盾的，
两个方向的研究进展最终都可以通过相互结合来创造出更强大的智能体。

除此之外，本文并未能成功地将蒙特卡洛树搜索应用于炉石传说，主要的瓶颈在于炉石传说一个回合可进行的操作总数可由于可操作实体数量的上升暴增到数万以上，
严重影响了蒙塔卡洛树扩展的效率，使得蒙特卡洛树搜索无法在可接受的时限内完成足够数量的迭代。未来需要考虑为炉石传说设计一套启发式的操作枚举系统，以期能以更高地效率排除价值较低的操作，
进而使得蒙特卡洛树搜索能在可接受的时限内完成迭代。
