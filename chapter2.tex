\section{蒙特卡洛搜索}
\label{section:MonteCarloSearch}
\subsection{蒙特卡洛方法}
\label{section:MonteCarloMethods}

蒙特卡洛方法（Monte Carlo methods）实际上为一类计算算法，这些算法依赖于重复的随机采样来求取数值结果。
蒙特卡洛方法主要起源于统计物理领域，被用于求出某些不可解的积分方程的近似值，现多被用于求解那些无法通过数学方法求解的物理或数学问题，
而其重复采样求取近似值的思想更是被延伸到了其他各种领域。

Abramson\cite{abramson1990expected}首次展示了蒙特卡洛方法的随机采样可以被用于近似某个对弈决策的理论收益。
使用Gelly和Silver所提出的代数符号\cite{gelly2011monte}，那么某个对弈操作的期望收益可以表示为$Q$值如下：
\begin{equation}
Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^{N(s)}\mathbb{I}_i (s,a)z_i
\end{equation}
其中$N(s,a)$为操作$a$在状态$s$下被选择的次数，$N(s)$为游戏从状态$s$被模拟的总次数，$z_i$为从状态$s$开始的第$i$次模拟的结果；当操作$a$在第$i$次模拟被从状态$s$选中时，$\mathbb{I}_i (s,a)$为$1$，否则为$0$。

有一些蒙特卡洛方法在给定的对弈状态下会对所有的可以进行的操作进行均匀采样，这样的蒙特卡洛方法被称为平蒙特卡洛方法（flat Monte Carlo）。
Ginsberg曾利用这样的蒙特卡洛方法开发出了能与世界级选手相竞的桥牌人工智能\cite{ginsberg2001gib}，可见这样的蒙特卡洛方法已有一定的实力。
这样的方法的不足之处也是显而易见的：它完全无法对对手的行为进行任何预测\cite{browne2011dangers}。
然而，在模拟时根据以往的经验偏向某些操作是完全可以提高所得的操作期望收益的准确度的。根据已有的操作期望收益，我们完全可以选择偏向那些拥有较高收益的操作。

\subsection{赌博机方法}
\label{section:BanditBasedMethods}

赌博机问题（bandit problems）是概率论中的经典问题。
问题给定$K$个操作（例如，$K$个不同的赌博机），要求参与者在这些操作中进行选择，以使得在重复选取最优收益后获取最高的累计收益。
每个操作所对应的收益分布是未知的，这使得参与者难以在这些操作中进行选择，而他只能通过曾经进行的尝试的结果来估算每个操作背后可能对应的收益。
这就产生了经典的穷尽-探索困境（exploitation-exploration dillemma）：
参与者必须在“穷尽（exploitation）当前已知的最优操作”与“探索（exploration）其他现在不是最优但有可能在多次模拟后成为最优操作的操作”之间进行取舍。

一个$K$臂赌博机可被定义为随机变量$X_{i,n}$，其中$i \in [1, K]$代表第$i$个赌博机（赌博机的第$i$个“臂”）\cite{auer2002finite,kocsis2006bandit,kocsis2006improved}，
$X_{i,1}, X_{i,2}, \ldots$依次为尝试第$i$个赌博机时得到的结果，它们之间相互独立且一致地遵循着某个未知的分布法则，且有着未知的期望值$\mu _{i}$。
通过定义一个可以根据赌博机过往给出的奖励决定该尝试哪个赌博机的策略（policy）可以解决$K$臂赌博机问题。

\subsubsection{后悔程度}
\label{section:reget}

给定的策略应能使得玩家的后悔程度（regret）最小。在$n$次尝试后，玩家的后悔程度可定义如下：
\begin{equation}
R_N = \mu ^* n - \mu _j \sum_{j=1}^K\mathbb{E}[T_j(n)]
\end{equation}
其中$\mu ^*$为期望收益最高的赌博机的期望收益，$\mathbb{E}[T_j(n)]$代表在这$n$次尝试中尝试了第$j$个赌博机的期望次数。
换句话说，玩家的后悔程度可以被理解为因没能尝试收益最高的赌博机所带来的损失期望。
值得注意是，无论为任何时候，任何一个赌博机被选中的几率都不能为零，否则收益最高的赌博机可能会因为其他暂时拥有较高收益的赌博机而被搜索算法所忽略。
为了保证这一点，我们应为每一个赌博机所观察到的收益值加上置信上界（upper confidence bound）。

\subsubsection{置信上界}
\label{section:UpperConfidenceBound}

为了解决$K$臂赌博机问题，搜索算法有必要引入置信上界，因为在任何时候，任何一个赌博机都可能是最优的赌博机。
由Auer et al提出的名为UCB1的策略\cite{auer2002finite}可以在未预先设置任何与收益分布有关的启发知识的情况下使玩家的后悔程度随尝试次数$n$呈对数级增长（$O(\ln n)$）。
该策略在每次选取赌博机时都会按照给定的公式为每个赌博机计算其对应的值，并选取所对应的值最大的赌博机。该公式定义如下：
\begin{equation}
UCB1 = \bar{X}_j + \sqrt{\frac{2\ln n}{n_j}}
\end{equation}
其中$\bar{X}_j$为第$j$个赌博机的平均收益，$n_j$为第$j$个赌博机被尝试的次数，$n$为尝试的总次数。
不难看出，收益项$\bar{X}_j$鼓励搜索算法穷尽（exploitation）目前平均收益最高的赌博机，而$\sqrt{\frac{2\ln n}{n_j}}$项则鼓励搜索算法探索（exploration）其他尝试次数较少的赌博机。
