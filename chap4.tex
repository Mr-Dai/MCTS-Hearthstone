\chapter{实验结果}
\label{section:experimentalResults}

本论文的实验将测试\ref{section:RuleBasedAIPlayers}节和\ref{section:MonteCarloAIPlayers}节提到的三个智能体，分别为随机智能体、基于规则智能体和蒙特卡洛智能体。
智能体将使用同一副卡组进行对弈，以免不同卡组之间的强度差异导致实验结果发生偏移。测试卡组的组成已在表\ref{table:TestDeck}中给出。
除此之外，随机智能体和规则智能体将会分别作为蒙特卡洛智能体的模拟规则，以测试启发式的先验知识是否能提高蒙特卡洛智能体的决策强度。

\begin{table}[!ht]
\small
\center
\caption{测试智能体}
\label{table:TestAi}
\begin{tabular}{|l|l|}
\hline
智能体名称    & 智能体描述                                       \\
\hline
Random        & 完全随机的智能体                                 \\
\hline
Rule-based    & 基于预定义规则的智能体                           \\
\hline
MC Random     & 使用随机智能体进行模拟的基于蒙特卡洛搜索的智能体 \\
\hline
MC Rule-based & 使用规则智能体进行模拟的基于蒙特卡洛搜索的智能体 \\
\hline
\end{tabular}
\end{table}

\section{智能体之间的决策强度差异}
\label{section:Test1}

本实验将用于测试不同智能体之间的决策强度的差异。智能体们将会进行相互对战。每组智能体各进行200次对战，并最终给出各个智能体在各种不同的组合中的胜场数。
其中，蒙特卡洛智能体默认在对所有操作进行一次尝试后会额外进行500次迭代。
测试时使用的计算机\footnote{Intel Core i5-3210M @ 2.50GHz 2.50GHz}可以在1秒内完成500次的模拟游戏。
后面的实验结果也表明500次的迭代已足以让蒙特卡洛智能体做出足够强度的决策。
本实验的结果也将用作后续实验的基准数据。

\begin{table}[!ht]
\small
\center
\caption{智能体对弈胜场数}
\label{table:Test1Results}
\begin{tabular}{|l|c|c|c|c|}
\hline
VS.           & Random & Rule-based & MC Random & MC Rule-based \\
\hline
Random        & 100    & 2          & 0         & 0             \\
\hline
Rule-based    & 198    & 100        & 88        & 60            \\
\hline
MC Random     & 200    & 112        & 100       & 140             \\
\hline
MC Rule-based & 200    & 140        & 60        & 100           \\
\hline
\end{tabular}
\end{table}

表\ref{table:Test1Results}给出了此次实验的结果。不难看出，随机智能体比起规则智能体和蒙特卡洛智能体都弱得多。
基于随机模拟的蒙特卡洛智能体在面对规则智能体时可以有56\%的胜率，可见即使是未添加任何先验知识的蒙特卡洛智能体已能与拥有大量先验知识的规则智能体并驾齐驱。
值得注意的是，当采用规则智能体进行模拟时，蒙特卡洛智能体拥有了来自规则智能体的先验知识，同时也拥有了蒙特卡洛搜索返回最优解的保证，
因此可以看到，基于规则模拟的蒙特卡洛智能体在面对规则智能体时的胜率比起基于随机模拟的蒙特卡洛智能体有了显著的提升（提升至70\%）。
更为有趣的是，基于规则模拟的蒙特卡洛智能体与基于随机模拟的蒙特卡洛智能体直接对战时，基于规则的智能体反而处于劣势。

\section{迭代次数对蒙特卡洛智能体决策强度的影响}
\label{section:Test2}

由第\ref{section:MonteCarloSearch}章可知，蒙特卡洛搜索通过不断地重复采样以近似每个操作的期望收益，采样次数越多意味着得到的期望收益越准确。
该过程类似于近似值逐渐逼近正确值的过程，理论上可知当采样次数达到一定程度后，采样次数的提高所能带来的可靠性的提高会变得微乎其微。
本次实验将调整上述实验中使用的两个蒙特卡洛智能体的迭代次数，以测试蒙特卡洛智能体在模拟次数逐渐增加时会有怎样的表现。

\begin{figure}[!ht]
\centering
\includegraphics[width=6in,height=3in]{img/Fig4-1.jpg}
\caption{基于规则模拟的蒙特卡洛智能体 vs. 规则智能体}
\label{fig:41}
\end{figure}

图\ref{fig:41}为基于规则模拟的蒙特卡洛智能体与规则智能体进行对战时，蒙特卡洛智能体的胜场数的变化。尽管变化不是很大，但仍能清楚地看到，随着迭代次数的增多，
蒙特卡洛智能体的胜场数有上升的趋势。蒙特卡洛智能体的胜场数在迭代数超过100后趋于稳定，稳定在了70\%胜率附近。这个数字与\ref{section:Test1}节得出的胜率极为接近。

\begin{figure}[!ht]
\centering
\includegraphics[width=6in,height=3in]{img/Fig4-2.jpg}
\caption{基于随机模拟的蒙特卡洛智能体 vs. 规则智能体}
\label{fig:42}
\end{figure}

图\ref{fig:42}为基于随机模拟的蒙特卡洛智能体与规则智能体进行对战时，蒙特卡洛智能体的胜场数的变化。
同样，从图中可以清晰地看到，蒙特卡洛智能体的胜场数在随着迭代次数的增多而增多。
尽管由于测试对弈的次数不足导致曲线有明显的波动，但仍可以看到，在迭代次数达到500以上时，蒙特卡洛智能体的胜场数开始趋于稳定，稳定在65\%胜率附近。

\begin{figure}[!ht]
\centering
\includegraphics[width=6in,height=3in]{img/Fig4-3.jpg}
\caption{基于规则模拟的蒙特卡洛智能体 vs. 基于随机模拟的蒙特卡洛智能体}
\label{fig:43}
\end{figure}

图\ref{fig:43}为规则蒙特卡洛智能体与随机蒙特卡洛智能体进行对战时，规则蒙特卡洛智能体的胜场数的变化。
与猜想的结果截然不同的是，随着两个智能体的迭代次数的增加，规则蒙特卡洛智能体的胜场数在逐渐下降。
当迭代次数达到500以上时，规则蒙特卡洛智能体的胜率已经下降到了35\%附近。

以上三次对弈的结果显示出了十分有趣的规律。首先比起随机蒙特卡洛智能体，规则蒙特卡洛智能体在与规则智能体对弈时，
其胜率可以以更少的迭代次数（100）稳定在较高的胜率（70\%）。尽管随机蒙特卡洛智能体表现不如规则蒙特卡洛智能体，
但随机蒙特卡洛智能体也在一定的迭代次数后（500），胜场数也趋于稳定（65\%）。
更为有趣的是，规则蒙特卡洛智能体与随机蒙特卡洛智能体对弈时，规则蒙特卡洛智能体反而处于劣势，且随着迭代次数的增加，差距仍在逐渐扩大。

蒙特卡洛搜索算法本身无法对对手的行为作出任何预测。完全随机的蒙特卡洛智能体由于不包含任何启发知识，在无法对对手的行为作出预测的情况下，
它得出的操作序列的期望收益可能是不准确的。但随着迭代次数的增加，大量的随机采样往往能保证期望收益最终趋近于真实值。
大概也正因如此，随机模拟的蒙特卡洛智能体需要更多的迭代来让胜场数趋于稳定，稳定后的胜场数也与规则蒙特卡洛智能体较为接近。
规则蒙特卡洛智能体由于加入了启发知识，它能更好地预测规则智能体的行为。但正是由于启发知识的加入，规则蒙特卡洛智能体的迭代采样不是随机的，
这就导致了它得出的期望收益是有偏差的，不适用于所有类型的对手，而随机蒙特卡洛智能体没有这样的局限。因此随着迭代次数的增加，
规则蒙特卡洛智能体渐渐被随机蒙特卡洛智能体超越，且差距越来越大。
